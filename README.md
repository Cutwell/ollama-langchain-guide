# <img src="resources/ollama.png" style="height:32px;margin-bottom:-8px;"> + <img src="local_langchain_tutorial/src/phi.svg" style="height:32px;margin-bottom:-8px;"> ‚á® <img src="resources/langchain.png" style="height:32px;padding-right:20px;margin-bottom:-8px;"> Local LangChain Tutorial
 Develop LangChain using local LLMs with Ollama

* LLM costs getting you down?
* Want to develop offline?
* Don't want to share your personal data with LLM providers?
* Save costs, develop anywhere, and own all your data with Ollama and LangChain!

## Before you start

* This tutorial requires several terminals to be open and running proccesses at once i.e.: to run various Ollama servers.
* When you see the üÜï emoji before a set of terminal commands, open a new terminal process.
* When you see the ‚ôªÔ∏è emoji before a set of terminal commands, you can re-use the same terminal you used last time.

## Prerequisites

1. Download and install [Ollama](https://ollama.com/download) and start the server.

üÜï
```sh
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
```

2. Download and install [Poetry](https://python-poetry.org/docs/#installing-with-the-official-installer).

3. Fork this repository and setup the Poetry environment:

üÜï
```sh
git clone https://github.com/Cutwell/local-langchain-tutorial.git
cd local-langchain-tutorial
poetry install
```

## Tutorial

1. Browse the [available Ollama models](https://ollama.com/library) and select a model.

* Think about your local computers available RAM and GPU memory when picking the model + quantisation level.
* We will be using the `phi-2` model from Microsoft ([Ollama](https://ollama.com/library/phi), [Hugging Face](https://huggingface.co/microsoft/phi-2)) as it is both small and fast.
* Read [this](https://www.promptingguide.ai/models/phi-2) summary for advice on prompting the `phi-2` model optimally.

‚ôªÔ∏è
```sh
ollama pull phi
```

2. Start the Ollama server.

* This server can be queried with LangChain, or you can interact with it in this terminal.

‚ôªÔ∏è
```sh
ollama run phi
```

3. Run the PyTest tests in `/local_langchain_tutorial/tests` to check everything is working correctly.

üÜï
```sh
poetry run pytest -rP local_langchain_tutorial/tests
```

4. Get started building your own local LLM projects with the example StreamLit app in `/local_langchain_tutorial/src`.

‚ôªÔ∏è
```sh
poetry run streamlit run local_langchain_tutorial/src/app.py --server.port=8080
```


## License
MIT
